{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Function\n",
    "Sigmoid functions are used for clasifications of two classes, these function may have any value between 0 and 1. We use it to convert numbers to probablities. They also have several desirable properties for traning neural networks.\n",
    "Mathamatical Equation of Sigmoid Function : <img src=\"img/math_sigmoid.png\">\n",
    "<img src=\"img/sigmoid.png\">\n",
    "One of the desirable properties of a sigmoid function is that its output can be used to create its derivative. If the sigmoid's output is a variable \"out\", then the derivative is simply out * (1-out). This is very efficient. \n",
    "If you're unfamililar with derivatives, just think about it as the slope of the sigmoid function at a given point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Dataset\n",
    "Initialize our input dataset as numpy matrix.Each rows is a single \"traning example\". Each nodes corresponds to one of our input nodes.Thus we have 3 input nodes to the network and 4 training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Dataset\n",
    "This initialize our output dataset. In this case, we generated dataset horizontally (With a single row and 4 coloum)for space. **\".T\"**  is the transpose function. After the transpose, this y matrix has 4 rows with one column. Just like our input, each row is a training example, and each column (only one) is an output node. So, our network has 3 inputs and 1 output and storing output as\n",
    "<img src=\"img/matrix.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Matrix\n",
    "This is our weight matrix for this neural network. It's called \"syn0\" to imply \"synapse zero\" (<font color=\"green\">_**Synapse** : In the nervous system, a synapse is a structure that permits a neuron (or nerve cell) to pass an electrical or chemical signal to another neuron or to the target efferent cell._</font>)\n",
    "\n",
    "Since we only have 2 layers (input and output), we only need one matrix of weights to connect them. Its dimension is (3,1) because we have 3 inputs and 1 output. Another way of looking at it is that l0 is of size 3 and l1 is of size 1. Thus, we want to connect every node in l0 to every node in l1, which requires a matrix of dimensionality (3,1). :) \n",
    "\n",
    "Another note is that the \"neural network\" is really just this matrix. We have \"layers\" l0 and l1 but they are transient values based on the dataset. We don't save them. All of the learning is stored in the syn0 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0 = 2*np.random.random((3,1)) -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **l1_delta** by using \"Error Weighted Derivative\" \n",
    "Formula : <img src=\"img/error_weight_der.png\">\n",
    "    So by substuting **SigmoidCurveGradient**(in our case nonlin(X,deriv=True)) we get \n",
    "    <img src=\"img/adjust_weight.png\">\n",
    "\n",
    "\n",
    "Some Methods used from numpy is as follows : \n",
    "-  exp     : the natural exponential\n",
    "-  array   :  creates a matrix\n",
    "-  dot     : multiplies matrices\n",
    "-  random  :  gives us random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training\n",
      "[[0.03178421]\n",
      " [0.02576499]\n",
      " [0.97906682]\n",
      " [0.97414645]]\n"
     ]
    }
   ],
   "source": [
    "for itr in range(1000):\n",
    "    # forward prapogation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    \n",
    "    #how much did we miss\n",
    "    l1_error = y - l1\n",
    "    #multiply how much we missed by the slope\n",
    "    # of the sigmoid at the value l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "    \n",
    "    #update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "    \n",
    "print(\"Output after training\")\n",
    "print(l1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
